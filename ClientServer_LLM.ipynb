{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttODwCFd8K0Q"
      },
      "source": [
        "# Access the MentalRiskEs data and interact with the server\n",
        "\n",
        "This notebook has been developed by the [SINAI](https://sinai.ujaen.es/) research group for its usage in the [MentalRiskES](https://sites.google.com/view/mentalriskes2025/) evaluation campaign at IberLEF 2025.\n",
        "\n",
        "**NOTE 1**: Please visit the [MentalRiskES competition website](https://sites.google.com/view/mentalriskes2025/evaluation) to read the instructions about how to download the data and interact with the server to send the predictions of your system.\n",
        "\n",
        "**NOTE 2**: Along the code, please replace \"URL\" by the URL server and \"TOKEN\" by your personal token.\n",
        "\n",
        "Remember this is a support to help you to develop your own system of communication with our server. We recommend you to download it as a Python script instead of working directly on colab and adapt the code to your needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DJN0pXx8W3-"
      },
      "source": [
        "# Install CodeCarbon package\n",
        "Read the [documentation](https://mlco2.github.io/codecarbon/) about the library if necessary. Remember that we provide a [CodeCarbon notebook](https://colab.research.google.com/drive/1boavnGOir0urui8qktbZaOmOV2pS5cn6?usp=sharing) with the example in its specific use in our competition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdvPWyc6x9cV",
        "outputId": "e4079235-d9e5-4302-cdda-c2d9b70ea6de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting codecarbon\n",
            "  Downloading codecarbon-2.8.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting arrow (from codecarbon)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from codecarbon) (8.1.8)\n",
            "Collecting fief-client[cli] (from codecarbon)\n",
            "  Downloading fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from codecarbon) (2.2.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.21.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from codecarbon) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from codecarbon) (9.0.0)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from codecarbon) (12.0.0)\n",
            "Collecting questionary (from codecarbon)\n",
            "  Downloading questionary-2.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting rapidfuzz (from codecarbon)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from codecarbon) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from codecarbon) (13.9.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from arrow->codecarbon) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->codecarbon)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting httpx<0.28.0,>=0.21.3 (from fief-client[cli]->codecarbon)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon)\n",
            "  Downloading jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting yaspin (from fief-client[cli]->codecarbon)\n",
            "  Downloading yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon) (2025.2)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->codecarbon) (12.570.86)\n",
            "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from questionary->codecarbon) (3.0.50)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer->codecarbon) (4.13.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->codecarbon) (1.5.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
            "Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.11/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.17.0)\n",
            "Collecting termcolor<2.4.0,>=2.2.0 (from yaspin->fief-client[cli]->codecarbon)\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n",
            "Downloading codecarbon-2.8.3-py3-none-any.whl (516 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.7/516.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading questionary-2.1.0-py3-none-any.whl (36 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Downloading fief_client-0.20.0-py3-none-any.whl (20 kB)\n",
            "Downloading yaspin-3.1.0-py3-none-any.whl (18 kB)\n",
            "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: types-python-dateutil, termcolor, rapidfuzz, yaspin, questionary, httpx, arrow, jwcrypto, fief-client, codecarbon\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 3.0.1\n",
            "    Uninstalling termcolor-3.0.1:\n",
            "      Successfully uninstalled termcolor-3.0.1\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.9.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arrow-1.3.0 codecarbon-2.8.3 fief-client-0.20.0 httpx-0.27.2 jwcrypto-1.5.6 questionary-2.1.0 rapidfuzz-3.13.0 termcolor-2.3.0 types-python-dateutil-2.9.0.20241206 yaspin-3.1.0\n",
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, dotenv\n",
            "Successfully installed dotenv-0.9.9 python-dotenv-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install codecarbon\n",
        "!pip install dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqyN-7TcXbL8"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Sqih7m6tN4MT"
      },
      "outputs": [],
      "source": [
        "import requests, zipfile, io\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "from typing import List, Dict\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import pandas as pd\n",
        "from codecarbon import EmissionsTracker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"SERVER_URL\"] = \"http://s3-ceatic.ujaen.es:8036\"\n",
        "os.environ[\"ACCESS_TOKEN\"] = \"c461869975ffb0a7ba8544ffdddf3b58\""
      ],
      "metadata": {
        "id": "likadYwTrvP_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHGGrr3GXdIb"
      },
      "source": [
        "# Endpoints\n",
        "These URL addresses are necessary for the connection to the server.\n",
        "\n",
        "**IMPORTANT:** Replace \"URL\" by the URL server and \"TOKEN\" by your user token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AdQPl8lbOKsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05335696-bc87-41f2-f5ed-b0f63392abd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://s3-ceatic.ujaen.es:8036 c461869975ffb0a7ba8544ffdddf3b58\n"
          ]
        }
      ],
      "source": [
        "load_dotenv()\n",
        "URL = os.getenv(\"SERVER_URL\")\n",
        "TOKEN = os.getenv(\"ACCESS_TOKEN\")\n",
        "print(URL, TOKEN)\n",
        "# Download endpoints\n",
        "ENDPOINT_DOWNLOAD_TRIAL = URL+\"/{TASK}/download_trial/{TOKEN}\"\n",
        "ENDPOINT_DOWNLOAD_TRAIN = URL+\"/{TASK}/download_train/{TOKEN}\"\n",
        "\n",
        "# Trial endpoints\n",
        "ENDPOINT_GET_MESSAGES_TRIAL = URL+\"/{TASK}/getmessages_trial/{TOKEN}\"\n",
        "ENDPOINT_SUBMIT_DECISIONS_TRIAL = URL+\"/{TASK}/submit_trial/{TOKEN}/{RUN}\"\n",
        "\n",
        "# Test endpoints\n",
        "ENDPOINT_GET_MESSAGES = URL+\"/{TASK}/getmessages/{TOKEN}\"\n",
        "ENDPOINT_SUBMIT_DECISIONS = URL+\"/{TASK}/submit/{TOKEN}/{RUN}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgHNiyxHR5AJ"
      },
      "source": [
        "# Download Data\n",
        "To download the data, you can make use of the **function defined in the following**.\n",
        "\n",
        "The following function download the trial data. To adapt it to download the train and test data, follow the instructions given in the [website of the competition](https://sites.google.com/view/mentalriskes2024/evaluation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Uaeh23C5R1lG"
      },
      "outputs": [],
      "source": [
        "def download_messages_trial(task: str, token: str):\n",
        "    \"\"\" Allows you to download the trial data of the task.\n",
        "        Args:\n",
        "          task (str): task from which the data is to be retrieved\n",
        "          token (str): authentication token\n",
        "    \"\"\"\n",
        "\n",
        "    response = requests.get(ENDPOINT_DOWNLOAD_TRIAL.format(TASK=task, TOKEN=token))\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(\"Trial - Status Code \" + task + \": \" + str(response.status_code) + \" - Error: \" + str(response.text))\n",
        "    else:\n",
        "      z = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "      os.makedirs(\"./data/{task}/trial/\".format(task=task))\n",
        "      z.extractall(\"./data/{task}/trial/\".format(task=task))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v09j7ChSD6c-"
      },
      "outputs": [],
      "source": [
        "def download_messages_train(task: str, token: str):\n",
        "    \"\"\" Allows you to download the train data of the task.\n",
        "        Args:\n",
        "          task (str): task from which the data is to be retrieved\n",
        "          token (str): authentication token\n",
        "    \"\"\"\n",
        "    response = requests.get(ENDPOINT_DOWNLOAD_TRAIN.format(TASK=task, TOKEN=token))\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(\"Train - Status Code \" + task + \": \" + str(response.status_code) + \" - Error: \" + str(response.text))\n",
        "    else:\n",
        "      z = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "      os.makedirs(\"./data/{task}/train/\".format(task=task),exist_ok=True)\n",
        "      z.extractall(\"./data/{task}/train/\".format(task=task))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIqRCv3OS3Bn"
      },
      "source": [
        "# Client Server\n",
        "This class simulates communication with our server. The following code established the conection with the server client and simulate the GET and POST requests.\n",
        "\n",
        "**IMPORTANT NOTE:** Please pay attention to the basic functions and remember that it is only a base for your system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "l0kONpltS2R9"
      },
      "outputs": [],
      "source": [
        "class Client_task1_2:\n",
        "    \"\"\" Client communicating with the official server.\n",
        "        Attributes:\n",
        "            token (str): authentication token\n",
        "            number_of_runs (int): number of systems. Must be 3 in order to advance to the next round.\n",
        "            tracker (EmissionsTracker): object to calculate the carbon footprint in prediction\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, task:str, token: str, number_of_runs: int, tracker: EmissionsTracker):\n",
        "        self.task = task\n",
        "        self.token = token\n",
        "        self.number_of_runs = number_of_runs\n",
        "        self.tracker = tracker\n",
        "        self.relevant_cols = ['duration', 'emissions', 'cpu_energy', 'gpu_energy',\n",
        "                              'ram_energy','energy_consumed', 'cpu_count', 'gpu_count',\n",
        "                              'cpu_model', 'gpu_model', 'ram_total_size','country_iso_code']\n",
        "\n",
        "\n",
        "    def get_messages(self, retries: int, backoff: float) -> Dict:\n",
        "        \"\"\" Allows you to download the test data of the task by rounds.\n",
        "            Here a GET request is sent to the server to extract the data.\n",
        "            Args:\n",
        "              retries (int): number of calls on the server connection\n",
        "              backoff (float): time between retries\n",
        "        \"\"\"\n",
        "        session = requests.Session()\n",
        "        retries = Retry(\n",
        "                        total = retries,\n",
        "                        backoff_factor = backoff,\n",
        "                        status_forcelist = [500, 502, 503, 504]\n",
        "                        )\n",
        "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "        response = session.get(ENDPOINT_GET_MESSAGES_TRIAL.format(TASK=self.task, TOKEN=self.token)) # ENDPOINT\n",
        "\n",
        "        if response.status_code != 200:\n",
        "          print(\"GET - Task {} - Status Code {} - Error: {}\".format(self.task, str(response.status_code), str(response.text)))\n",
        "          return []\n",
        "        else:\n",
        "          return json.loads(response.content)\n",
        "\n",
        "    def submit_decission(self, messages: List[Dict], emissions: Dict, retries: int, backoff: float):\n",
        "        \"\"\" Allows you to submit the decisions of the task by rounds.\n",
        "            The POST requests are sent to the server to send predictions and carbon emission data\n",
        "            Args:\n",
        "              messages (List[Dict]): Message set of the current round\n",
        "              emissions (Dict): carbon footprint generated in the prediction\n",
        "              retries (int): number of calls on the server connection\n",
        "              backoff (float): time between retries\n",
        "        \"\"\"\n",
        "        decisions_run0 = {}\n",
        "        decisions_run1 = {}\n",
        "        decisions_run2 = {}\n",
        "        type_addiction_list = [\"betting\", \"onlinegaming\", \"betting\", \"trading\"]\n",
        "        type_addiction_decision = {}\n",
        "\n",
        "        # You must create the appropriate structure to send the predictions according to each task\n",
        "        for message in messages:\n",
        "            decisions_run0[message[\"nick\"]] = random.choice([0,1])\n",
        "            decisions_run1[message[\"nick\"]] = random.choice([0,1])\n",
        "            decisions_run2[message[\"nick\"]] = random.choice([0,1])\n",
        "            type_addiction_decision[message[\"nick\"]] = random.choice(type_addiction_list)\n",
        "\n",
        "        data1_run0 = {\n",
        "            \"predictions\": decisions_run0,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "        data1_run1 = {\n",
        "            \"predictions\": decisions_run1,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "        data1_run2 = {\n",
        "            \"predictions\": decisions_run2,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "        data2_run0 = {\n",
        "            \"predictions\": decisions_run0,\n",
        "            \"types\":type_addiction_decision,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "        data2_run1 = {\n",
        "            \"predictions\": decisions_run1,\n",
        "            \"types\":type_addiction_decision,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "        data2_run2 = {\n",
        "            \"predictions\": decisions_run2,\n",
        "            \"types\":type_addiction_decision,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "\n",
        "        data1 = []\n",
        "        data1.append(json.dumps(data1_run0))\n",
        "        data1.append(json.dumps(data1_run1))\n",
        "        data1.append(json.dumps(data1_run2))\n",
        "\n",
        "        data2 = []\n",
        "        data2.append(json.dumps(data2_run0))\n",
        "        data2.append(json.dumps(data2_run1))\n",
        "        data2.append(json.dumps(data2_run2))\n",
        "\n",
        "        # Session to POST request\n",
        "        session = requests.Session()\n",
        "        retries = Retry(\n",
        "                        total = retries,\n",
        "                        backoff_factor = backoff,\n",
        "                        status_forcelist = [500, 502, 503, 504]\n",
        "                        )\n",
        "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "        for run in range(0, self.number_of_runs):\n",
        "            # For each run, new decisions\n",
        "            response1 = session.post(ENDPOINT_SUBMIT_DECISIONS_TRIAL.format(TASK='task1', TOKEN=self.token, RUN=run), json=[data1[run]]) # ENDPOINT\n",
        "            if response1.status_code != 200:\n",
        "                print(\"POST - Task1 - Status Code {} - Error: {}\".format(str(response1.status_code), str(response1.text)))\n",
        "                return\n",
        "            else:\n",
        "                print(\"POST - Task1 - run {} - Message: {}\".format(run, str(response1.text)))\n",
        "\n",
        "            response2 = session.post(ENDPOINT_SUBMIT_DECISIONS_TRIAL.format(TASK='task2', TOKEN=self.token, RUN=run), json=[data2[run]]) # ENDPOINT\n",
        "            if response2.status_code != 200:\n",
        "                print(\"POST - Task2 - Status Code {} - Error: {}\".format(str(response2.status_code), str(response2.text)))\n",
        "                return\n",
        "            else:\n",
        "                print(\"POST - Task2 - run {} - Message: {}\".format(run, str(response2.text)))\n",
        "\n",
        "            with open('./data/preds/task1/round{}_run{}.json'.format(messages[0][\"round\"], run), 'w+', encoding='utf8') as json_file:\n",
        "                json.dump(data1[run], json_file, ensure_ascii=False)\n",
        "            with open('./data/preds/task2/round{}_run{}.json'.format(messages[0][\"round\"], run), 'w+', encoding='utf8') as json_file:\n",
        "                json.dump(data2[run], json_file, ensure_ascii=False)\n",
        "\n",
        "\n",
        "    def run_task1_2(self, retries: int, backoff: float):\n",
        "        \"\"\" Main thread\n",
        "            Args:\n",
        "              retries (int): number of calls on the server connection\n",
        "              backoff (float): time between retries\n",
        "        \"\"\"\n",
        "        # Get messages for task1_2\n",
        "        messages = self.get_messages(retries, backoff)\n",
        "\n",
        "        # If there are no messages\n",
        "        if len(messages) == 0:\n",
        "            print(\"All rounds processed\")\n",
        "            return\n",
        "\n",
        "        while len(messages) > 0:\n",
        "            print(messages)\n",
        "            print(\"----------------------- Processing round {}\".format(messages[0][\"round\"]))\n",
        "            # Save subjects\n",
        "            with open('./data/rounds/round{}.json'.format(messages[0][\"round\"]), 'w+', encoding='utf8') as json_file:\n",
        "                json.dump(messages, json_file, ensure_ascii=False)\n",
        "\n",
        "            # Calculate emissions for each prediction\n",
        "            self.tracker.start()\n",
        "\n",
        "            # Your code\n",
        "\n",
        "            emissions = self.tracker.stop()\n",
        "            df = pd.read_csv(\"emissions.csv\")\n",
        "            measurements = df.iloc[-1][self.relevant_cols].to_dict()\n",
        "\n",
        "            self.submit_decission(messages, measurements, retries, backoff)\n",
        "\n",
        "            # One GET request for each round\n",
        "            messages = self.get_messages(retries, backoff)\n",
        "\n",
        "        print(\"All rounds processed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMXuHLciXIO3"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GZrDpxNAS6-3"
      },
      "outputs": [],
      "source": [
        "def download_data(task: str, token: str):\n",
        "    # download_messages_trial(task, token)\n",
        "    download_messages_train(task, token)\n",
        "\n",
        "def get_post_data(task: str, token: str):\n",
        "    # Emissions Tracker Config\n",
        "    config = {\n",
        "        \"save_to_file\": True,\n",
        "        \"log_level\": \"WARNING\",\n",
        "        \"tracking_mode\": \"process\",\n",
        "        \"output_dir\": \".\",\n",
        "        \"allow_multiple_runs\": True\n",
        "    }\n",
        "    tracker = EmissionsTracker(**config)\n",
        "\n",
        "    number_runs = 3 # Max: 3\n",
        "\n",
        "    # Prediction period\n",
        "    client_task1_2 = Client_task1_2(task, token, number_runs, tracker)\n",
        "    client_task1_2.run_task1_2(5, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff6QgM3gErMm"
      },
      "source": [
        "Be careful! In this specific example we use the name of the task1 to do the get, knowing that it is the same data for both task 1 and task 2. In addition, the data upload is performed for both tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aKMWQ5buS8OK"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    download_data(\"task2\", TOKEN)\n",
        "    # get_post_data(\"task1\",TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHzds8XvsRCe",
        "outputId": "b7a614c7-dead-4393-ce23-5dbacfea012c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.22.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
            "Downloading groq-0.22.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "import re"
      ],
      "metadata": {
        "id": "iIwK0NEBtU6N"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TASK_LLM:\n",
        "    \"\"\" Client communicating with the official server.\n",
        "        Attributes:\n",
        "            token (str): authentication token\n",
        "            number_of_runs (int): number of systems. Must be 3 in order to advance to the next round.\n",
        "            tracker (EmissionsTracker): object to calculate the carbon footprint in prediction\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, task:str, token: str, number_of_runs: int, tracker: EmissionsTracker):\n",
        "        self.task = task\n",
        "        self.token = token\n",
        "        self.number_of_runs = number_of_runs\n",
        "        self.tracker = tracker\n",
        "        self.relevant_cols = ['duration', 'emissions', 'cpu_energy', 'gpu_energy',\n",
        "                              'ram_energy','energy_consumed', 'cpu_count', 'gpu_count',\n",
        "                              'cpu_model', 'gpu_model', 'ram_total_size','country_iso_code']\n",
        "\n",
        "\n",
        "    def get_messages(self, retries: int, backoff: float) -> Dict:\n",
        "        \"\"\" Allows you to download the test data of the task by rounds.\n",
        "            Here a GET request is sent to the server to extract the data.\n",
        "            Args:\n",
        "              retries (int): number of calls on the server connection\n",
        "              backoff (float): time between retries\n",
        "        \"\"\"\n",
        "        session = requests.Session()\n",
        "        retries = Retry(\n",
        "                        total = retries,\n",
        "                        backoff_factor = backoff,\n",
        "                        status_forcelist = [500, 502, 503, 504]\n",
        "                        )\n",
        "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "        response = session.get(ENDPOINT_GET_MESSAGES_TRIAL.format(TASK=self.task, TOKEN=self.token)) # ENDPOINT\n",
        "\n",
        "        if response.status_code != 200:\n",
        "          print(\"GET - Task {} - Status Code {} - Error: {}\".format(self.task, str(response.status_code), str(response.text)))\n",
        "          return []\n",
        "        else:\n",
        "          return json.loads(response.content)\n",
        "\n",
        "    def submit_decission(self, messages: List[Dict], emissions: Dict, retries: int, backoff: float):\n",
        "        \"\"\" Allows you to submit the decisions of the task by rounds.\n",
        "            The POST requests are sent to the server to send predictions and carbon emission data\n",
        "            Args:\n",
        "              messages (List[Dict]): Message set of the current round\n",
        "              emissions (Dict): carbon footprint generated in the prediction\n",
        "              retries (int): number of calls on the server connection\n",
        "              backoff (float): time between retries\n",
        "        \"\"\"\n",
        "        decisions_run0 = {}\n",
        "        decisions_run1 = {}\n",
        "        decisions_run2 = {}\n",
        "        type_addiction_list = [\"betting\", \"onlinegaming\", \"betting\", \"trading\"]\n",
        "        type_addiction_decision = {}\n",
        "\n",
        "        # You must create the appropriate structure to send the predictions according to each task\n",
        "        for message in messages:\n",
        "            decisions_run0[message[\"nick\"]] = random.choice([0,1])\n",
        "            decisions_run1[message[\"nick\"]] = random.choice([0,1])\n",
        "            decisions_run2[message[\"nick\"]] = random.choice([0,1])\n",
        "            type_addiction_decision[message[\"nick\"]] = random.choice(type_addiction_list)\n",
        "\n",
        "        data1_run0 = {\n",
        "            \"predictions\": decisions_run0,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "        data1_run1 = {\n",
        "            \"predictions\": decisions_run1,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "        data1_run2 = {\n",
        "            \"predictions\": decisions_run2,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "        data2_run0 = {\n",
        "            \"predictions\": decisions_run0,\n",
        "            \"types\":type_addiction_decision,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "        data2_run1 = {\n",
        "            \"predictions\": decisions_run1,\n",
        "            \"types\":type_addiction_decision,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "        data2_run2 = {\n",
        "            \"predictions\": decisions_run2,\n",
        "            \"types\":type_addiction_decision,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "\n",
        "        data1 = []\n",
        "        data1.append(json.dumps(data1_run0))\n",
        "        data1.append(json.dumps(data1_run1))\n",
        "        data1.append(json.dumps(data1_run2))\n",
        "\n",
        "        data2 = []\n",
        "        data2.append(json.dumps(data2_run0))\n",
        "        data2.append(json.dumps(data2_run1))\n",
        "        data2.append(json.dumps(data2_run2))\n",
        "\n",
        "        # Session to POST request\n",
        "        session = requests.Session()\n",
        "        retries = Retry(\n",
        "                        total = retries,\n",
        "                        backoff_factor = backoff,\n",
        "                        status_forcelist = [500, 502, 503, 504]\n",
        "                        )\n",
        "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "        for run in range(0, self.number_of_runs):\n",
        "            # For each run, new decisions\n",
        "            response1 = session.post(ENDPOINT_SUBMIT_DECISIONS_TRIAL.format(TASK='task1', TOKEN=self.token, RUN=run), json=[data1[run]]) # ENDPOINT\n",
        "            if response1.status_code != 200:\n",
        "                print(\"POST - Task1 - Status Code {} - Error: {}\".format(str(response1.status_code), str(response1.text)))\n",
        "                return\n",
        "            else:\n",
        "                print(\"POST - Task1 - run {} - Message: {}\".format(run, str(response1.text)))\n",
        "\n",
        "            response2 = session.post(ENDPOINT_SUBMIT_DECISIONS_TRIAL.format(TASK='task2', TOKEN=self.token, RUN=run), json=[data2[run]]) # ENDPOINT\n",
        "            if response2.status_code != 200:\n",
        "                print(\"POST - Task2 - Status Code {} - Error: {}\".format(str(response2.status_code), str(response2.text)))\n",
        "                return\n",
        "            else:\n",
        "                print(\"POST - Task2 - run {} - Message: {}\".format(run, str(response2.text)))\n",
        "\n",
        "            with open('./data/preds/task1/round{}_run{}.json'.format(messages[0][\"round\"], run), 'w+', encoding='utf8') as json_file:\n",
        "                json.dump(data1[run], json_file, ensure_ascii=False)\n",
        "            with open('./data/preds/task2/round{}_run{}.json'.format(messages[0][\"round\"], run), 'w+', encoding='utf8') as json_file:\n",
        "                json.dump(data2[run], json_file, ensure_ascii=False)\n",
        "\n",
        "\n",
        "    def run_task1_2(self, retries: int, backoff: float):\n",
        "        \"\"\" Main thread\n",
        "            Args:\n",
        "              retries (int): number of calls on the server connection\n",
        "              backoff (float): time between retries\n",
        "        \"\"\"\n",
        "        # Get messages for task1_2\n",
        "        messages = self.get_messages(retries, backoff)\n",
        "\n",
        "        # If there are no messages\n",
        "        if len(messages) == 0:\n",
        "            print(\"All rounds processed\")\n",
        "            return\n",
        "\n",
        "        while len(messages) > 0:\n",
        "            print(messages)\n",
        "            print(\"----------------------- Processing round {}\".format(messages[0][\"round\"]))\n",
        "            # Save subjects\n",
        "            with open('./data/rounds/round{}.json'.format(messages[0][\"round\"]), 'w+', encoding='utf8') as json_file:\n",
        "                json.dump(messages, json_file, ensure_ascii=False)\n",
        "\n",
        "            # Calculate emissions for each prediction\n",
        "            self.tracker.start()\n",
        "\n",
        "            # Your code\n",
        "            prediction, addiction_type = self.predict_with_llama(messages)\n",
        "\n",
        "            emissions = self.tracker.stop()\n",
        "            df = pd.read_csv(\"emissions.csv\")\n",
        "            measurements = df.iloc[-1][self.relevant_cols].to_dict()\n",
        "\n",
        "            self.submit_decission(messages, measurements, retries, backoff)\n",
        "\n",
        "            # One GET request for each round\n",
        "            messages = self.get_messages(retries, backoff)\n",
        "\n",
        "        print(\"All rounds processed\")\n",
        "\n",
        "    def predict_with_llama(self, messages):\n",
        "\n",
        "        client = Groq(\n",
        "            api_key= os.getenv(\"GROQ_API_KEY\"),\n",
        "        )\n",
        "        # print (\"messages\", messages)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        A continuación se te dará una serie de mensajes escritos por una persona. Tu tarea es analizar el contenido y determinar si hay indicios de que la persona podría estar mostrando\n",
        "        comportamientos relacionados con la ludopatía (adicción al juego).\n",
        "\n",
        "        Tu respuesta debe ser exclusivamente un número:\n",
        "        - 1 si hay indicios de ludopatía. A continuación, en la siguiente línea, pon el tipo de entre: 'betting', 'onlinegaming', 'trading'\n",
        "        - 0 si no los hay.\n",
        "\n",
        "        Ahora evalúa dados los siguientes mensajes:\n",
        "        {messages}\n",
        "        \"\"\"\n",
        "\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt,\n",
        "                }\n",
        "            ],\n",
        "            model=\"llama3-70b-8192\",\n",
        "        )\n",
        "\n",
        "        response = chat_completion.choices[0].message.content\n",
        "        string_match = re.match(r\"(\\d+)\\s*(.*)\", response)\n",
        "        if string_match is None:\n",
        "          return 0\n",
        "\n",
        "        number = string_match.group(1)\n",
        "        type_addiction = string_match.group(2) or None\n",
        "\n",
        "        return int(number), type_addiction"
      ],
      "metadata": {
        "id": "1heULcoHsOlx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = TASK_LLM(\"LLM\", os.getenv(\"ACCESS_TOKEN\"), 3, EmissionsTracker(\"a\"))\n",
        "t1.run_task1_2(1,1.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I9AqXq3w23K",
        "outputId": "e60df5f5-8d0e-4edf-a8b9-766c59952755"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 23:39:26] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 23:39:26] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 23:39:26] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 23:39:27] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 23:39:27] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 23:39:27] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 23:39:27] No GPU found.\n",
            "[codecarbon INFO @ 23:39:27] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 23:39:27]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 23:39:27]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 23:39:27]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 23:39:27]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 23:39:27]   CPU count: 2\n",
            "[codecarbon INFO @ 23:39:27]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 23:39:27]   GPU count: None\n",
            "[codecarbon INFO @ 23:39:27]   GPU model: None\n",
            "[codecarbon INFO @ 23:39:27] Saving emissions data to file /content/emissions.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GET - Task LLM - Status Code 404 - Error: {\"detail\":\"Not Found\"}\n",
            "All rounds processed\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}