{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f1d2f1",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "This notebook contains the preprocessing pipeline tried (not much success) and the dataset translation functionality, it was implemented as an experiment checking whether translating would fix\n",
    "some of the typos and yield better results (not used because of bad performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f55d3f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxi.rodriguez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by GridSearchCV: {'C': 10}\n",
      "Mean SVM accuracy over 5 seeds: 0.6543\n",
      "Standard deviation of SVM accuracy over 5 seeds: 0.0246\n",
      "Mean SVM F1 score over 5 seeds: 0.6520\n",
      "Standard deviation of SVM F1 score over 5 seeds: 0.0265\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk import SnowballStemmer\n",
    "import spacy\n",
    "from nltk.tokenize import TweetTokenizer  # Import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from pathlib import Path\n",
    "import os\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from joblib import Parallel, delayed\n",
    "import json\n",
    "# Download necessary NLTK data\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"wordnet\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "\n",
    "# Load spaCy Spanish model\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading es_core_news_sm model for spaCy...\")\n",
    "    spacy.cli.download(\"es_core_news_sm\")\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "\n",
    "# --- DATA CLEANING FUNCTIONS ---\n",
    "def clean_keywords(keyword):\n",
    "    cleaned = re.sub(r'%20', ' ', keyword)\n",
    "    return cleaned\n",
    "\n",
    "def remove_accents(keyword):\n",
    "    normalized = unicodedata.normalize('NFD', keyword)\n",
    "    cleaned = ''.join([char for char in normalized if unicodedata.category(char) != 'Mn'])\n",
    "    return cleaned\n",
    "\n",
    "def remove_punctuation(keyword):\n",
    "    cleaned = re.sub(r\"[!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n -' ]\",\" \",keyword)\n",
    "    return cleaned\n",
    "\n",
    "def normalize_money(keyword):\n",
    "    money_symbols = [r'\\$', r'€', r'£', r'¥', r'₹', r'₣']\n",
    "    cleaned = keyword \n",
    "    for symbol in money_symbols:\n",
    "        cleaned = re.sub(symbol, \"dinero\", cleaned)\n",
    "    return cleaned\n",
    "\n",
    "def normalize_pct(keyword):\n",
    "    cleaned = re.sub(r\"%\", \" porcentaje\", keyword)\n",
    "    return cleaned\n",
    "\n",
    "def remove_jaja(keyword):\n",
    "    jaja_set = (\"jaja\", \"jeje\", \"jiji\", \"jojo\", \"juju\")\n",
    "    return \" \".join([m for m in keyword.split() if not any(ja_word in m for ja_word in jaja_set)])\n",
    "\n",
    "def is_exaggeration(word):\n",
    "    for c in word:\n",
    "        if len(c) == 1:\n",
    "            return False\n",
    "        if word.count(c) / len(word) > 0.6:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def remove_exaggerations(keyword):\n",
    "    return \" \".join([m for m in keyword.split() if not is_exaggeration(m)])\n",
    "\n",
    "def remove_hashtag(keyword):\n",
    "    cleaned = re.sub(r'INI_HASHTAG.*?END_HASHTAG', '', keyword)   \n",
    "    return cleaned\n",
    "\n",
    "def remove_unknown(keyword):\n",
    "    return \" \".join([m for m in keyword.split() if not \"unknown\" in m])\n",
    "\n",
    "def replace_multiplicador(input_string):\n",
    "    cleaned = re.sub(r'x(\\d+)', r'multiplicador \\1', input_string)\n",
    "    return cleaned\n",
    "\n",
    "def remove_user(input_string):\n",
    "    cleaned = re.sub(r'\\buser\\b', '', input_string)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def remove_extra_whitespaces(input_string):\n",
    "    cleaned = re.sub(r'\\s+', ' ', input_string).strip()\n",
    "    return cleaned\n",
    "\n",
    "def remove_numbers(keyword):\n",
    "    cleaned = re.sub(r'\\d+', '', keyword)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def remove_emojis(keyword):\n",
    "    return re.sub(r\"[^\\w\\s,!?@#áéíóúÁÉÍÓÚñÑ]\", \"\", keyword)\n",
    "\n",
    "def remove_single_chars(keyword):\n",
    "    return \" \".join([m for m in keyword.split() if len(m) > 2])\n",
    "\n",
    "def remove_consecutive_duplicates(word):\n",
    "    cleaned = re.sub(r\"(.)\\1+\", r\"\\1\", word)\n",
    "    return cleaned\n",
    "\n",
    "# --- SPELL CHECKER FUNCTIONS ---\n",
    "\n",
    "spell = SpellChecker(language='es')\n",
    "word_cache = {}  # Initialize the word cache\n",
    "\n",
    "def correct_spelling(keyword):\n",
    "    corrected_words = []\n",
    "    for word in keyword.split():\n",
    "        # Check if the word is already in the cache\n",
    "        if word in word_cache:\n",
    "            correction = word_cache[word]\n",
    "        else:\n",
    "            # Correct the word if not in the cache\n",
    "            correction = spell.correction(word)\n",
    "            if correction is None:  # Handle unknown words\n",
    "                correction = word  # Keep the original word\n",
    "            word_cache[word] = correction  # Store in the cache\n",
    "\n",
    "        corrected_words.append(correction)\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "def correct_spelling_parallel(keywords, n_jobs=-1):\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(correct_spelling)(keyword) for keyword in keywords)\n",
    "    return results\n",
    "\n",
    "# --- PIPELINE FUNCTIONS ---\n",
    "def clean_pipeline(input_string):\n",
    "    input_string = input_string.lower()\n",
    "    input_string = clean_keywords(input_string)\n",
    "    input_string = remove_emojis(input_string)\n",
    "    input_string = remove_hashtag(input_string)\n",
    "    input_string = normalize_money(input_string)\n",
    "    input_string = normalize_pct(input_string)\n",
    "    input_string = remove_jaja(input_string)\n",
    "    input_string = remove_consecutive_duplicates(input_string)\n",
    "    input_string = remove_unknown(input_string)\n",
    "    input_string = replace_multiplicador(input_string)\n",
    "    input_string = remove_punctuation(input_string)\n",
    "    input_string = remove_user(input_string)\n",
    "    input_string = remove_numbers(input_string)\n",
    "    input_string = remove_single_chars(input_string)\n",
    "    input_string = remove_extra_whitespaces(input_string)\n",
    "    input_string = remove_accents(input_string)\n",
    "    \n",
    "    return input_string\n",
    "\n",
    "def lemmatize_spanish(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# --- DATA LOADING FUNCTIONS ---\n",
    "\n",
    "def get_users_data(data_path: str, proccess_data: bool = True) -> tuple:\n",
    "    path = Path(data_path)\n",
    "    all_users_data = []\n",
    "    users_order = []\n",
    "    tokenizer = TweetTokenizer()  # Initialize TweetTokenizer\n",
    "    stop_words = set(stopwords.words('spanish'))  # Load Spanish stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for element in path.iterdir():\n",
    "        if element.is_file():\n",
    "            try:\n",
    "                user_id = re.findall(r\"[0-9]+\", element.name)[0]\n",
    "                users_order.append(user_id)\n",
    "            except IndexError:\n",
    "                print(f\"Warning: Could not extract user ID from filename: {element.name}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(path / element, 'r', encoding='utf-8') as json_file:  # Use Path object\n",
    "                    json_data = json.load(json_file)\n",
    "                    messages = [str(record['message']) for record in json_data]\n",
    "                    user_document = \" \".join(messages)  # Combine messages into a single document\n",
    "\n",
    "                    if proccess_data:\n",
    "                        user_document = clean_pipeline(user_document)\n",
    "                        # user_document = correct_spelling(user_document)  # Spelling correction\n",
    "                        user_document = lemmatize_spanish(user_document)\n",
    "                        tokens = tokenizer.tokenize(user_document)  # Tokenize the document\n",
    "                        tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]  # Lemmatize & remove stopwords\n",
    "                        user_document = \" \".join(tokens)  # Rejoin the tokens\n",
    "\n",
    "                    all_users_data.append(user_document)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {element.name}: {e}\")\n",
    "                continue\n",
    "    return all_users_data, users_order\n",
    "\n",
    "def tokenize_and_vectorize(data_path: str, ngram_range=(1, 1)) -> tuple:\n",
    "    \"\"\" Tokenizes and vectorizes messages with TF-IDF. \"\"\"\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "    all_users_data, users_order = get_users_data(data_path)\n",
    "\n",
    "    X = vectorizer.fit_transform(all_users_data)\n",
    "    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return df, users_order\n",
    "\n",
    "\n",
    "def analyze_sentiment(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Analyzes the sentiment of the given text using VADER and discretizes it into:\n",
    "    -1: Negative\n",
    "     0: Neutral\n",
    "     1: Positive\n",
    "    \"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(text)\n",
    "    compound_score = vs['compound']\n",
    "\n",
    "    if compound_score >= 0.05:\n",
    "        return 1  # Positive\n",
    "    elif compound_score <= -0.05:\n",
    "        return -1  # Negative\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def tokenize_and_vectorize_with_sentiment(data_path: str, ngram_range=(1, 1)) -> tuple:\n",
    "    \"\"\" Tokenizes, vectorizes with TF-IDF, and adds sentiment analysis. \"\"\"\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "    all_users_data, users_order = get_users_data(data_path)\n",
    "\n",
    "    X = vectorizer.fit_transform(all_users_data)\n",
    "    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    sentiment_scores = [analyze_sentiment(text) for text in all_users_data]\n",
    "    df['sentiment'] = sentiment_scores\n",
    "\n",
    "    return df, users_order\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Data loading\n",
    "    data_path = os.getcwd() + '/data/task1/train/subjects/'\n",
    "    df_vectorized, users_order = tokenize_and_vectorize_with_sentiment(data_path=data_path, ngram_range=(1, 2)) # Use bigrams\n",
    "\n",
    "    # Load target variable\n",
    "    target_path = os.getcwd() + '/data/task1/train/gold_task1.txt'\n",
    "    target_col = pd.read_csv(filepath_or_buffer=target_path, delimiter=',').to_numpy()\n",
    "    users_tags = {re.findall(r\"[0-9]+\", target_col[i][0])[0]: target_col[i][1] for i in range(len(target_col))}\n",
    "\n",
    "    # Create supervised dataframe\n",
    "    df_supervised = df_vectorized.copy()\n",
    "    target = np.array([users_tags[user_id] for user_id in users_order], np.int8)\n",
    "    df_supervised.insert(loc=len(df_supervised.columns), column='Target', value=target)\n",
    "\n",
    "    # --- MODEL TRAINING AND EVALUATION ---\n",
    "\n",
    "    def train_and_evaluate(seed, C=1.0): # Added C parameter\n",
    "        X, y = df_supervised.drop(columns=['Target'], axis=1), df_supervised['Target'].to_numpy()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y, shuffle=True)\n",
    "\n",
    "        clf = LinearSVC(dual=False, max_iter=10000, C=C, class_weight='balanced')  # Tune C, handle imbalance\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        acc = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "        f1 = f1_score(y_true=y_test, y_pred=y_pred, average='weighted') # Calculate F1 score\n",
    "\n",
    "        return acc, f1\n",
    "\n",
    "    # Hyperparameter tuning using GridSearchCV\n",
    "    X, y = df_supervised.drop(columns=['Target'], axis=1), df_supervised['Target'].to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y, shuffle=True)\n",
    "\n",
    "    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    grid_search = GridSearchCV(LinearSVC(dual=False, max_iter=10000, class_weight='balanced'), \n",
    "                                param_grid, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters found by GridSearchCV:\", grid_search.best_params_)\n",
    "    best_C = grid_search.best_params_['C']\n",
    "\n",
    "    # Train and evaluate the model with the best C parameter\n",
    "    num_seeds = 5 # Reduce number of seeds\n",
    "    seeds = [42, 123, 56, 789, 10]\n",
    "\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    for seed in seeds:\n",
    "        acc, f1 = train_and_evaluate(seed, C=best_C)\n",
    "        accuracies.append(acc)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "\n",
    "    print(f\"Mean SVM accuracy over {num_seeds} seeds: {mean_accuracy:.4f}\")\n",
    "    print(f\"Standard deviation of SVM accuracy over {num_seeds} seeds: {std_accuracy:.4f}\")\n",
    "    print(f\"Mean SVM F1 score over {num_seeds} seeds: {mean_f1:.4f}\")\n",
    "    print(f\"Standard deviation of SVM F1 score over {num_seeds} seeds: {std_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def parallel_translate_documents(documents, target_lang=\"en\", source_lang=\"auto\", max_workers=8, verbose=False):\n",
    "    \"\"\"Translate a list of documents in parallel using deep-translator and return them in the same order.\"\"\"\n",
    "    \n",
    "    def translate_single(index, text):\n",
    "        try:\n",
    "            translation = GoogleTranslator(source=source_lang, target=target_lang).translate(text)\n",
    "            return index, translation\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Translation failed for index {index}: {e}\")\n",
    "            return index, text  # Return original text if translation fails\n",
    "\n",
    "    translated_docs = [None] * len(documents)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(translate_single, i, doc) for i, doc in enumerate(documents)]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            index, translation = future.result()\n",
    "            translated_docs[index] = translation\n",
    "\n",
    "    return translated_docs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
