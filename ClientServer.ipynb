{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttODwCFd8K0Q"
      },
      "source": [
        "# Access the MentalRiskEs data and interact with the server\n",
        "\n",
        "This notebook has been developed by the [SINAI](https://sinai.ujaen.es/) research group for its usage in the [MentalRiskES](https://sites.google.com/view/mentalriskes2025/) evaluation campaign at IberLEF 2025.\n",
        "\n",
        "**NOTE 1**: Please visit the [MentalRiskES competition website](https://sites.google.com/view/mentalriskes2025/evaluation) to read the instructions about how to download the data and interact with the server to send the predictions of your system.\n",
        "\n",
        "**NOTE 2**: Along the code, please replace \"URL\" by the URL server and \"TOKEN\" by your personal token.\n",
        "\n",
        "Remember this is a support to help you to develop your own system of communication with our server. We recommend you to download it as a Python script instead of working directly on colab and adapt the code to your needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DJN0pXx8W3-"
      },
      "source": [
        "# Install CodeCarbon package\n",
        "Read the [documentation](https://mlco2.github.io/codecarbon/) about the library if necessary. Remember that we provide a [CodeCarbon notebook](https://colab.research.google.com/drive/1boavnGOir0urui8qktbZaOmOV2pS5cn6?usp=sharing) with the example in its specific use in our competition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdvPWyc6x9cV",
        "outputId": "cf639de5-3041-46ef-8d19-5049e02dc0d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, dotenv\n",
            "Successfully installed dotenv-0.9.9 python-dotenv-1.1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
            "[notice] To update, run: C:\\Users\\maxi.rodriguez\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# !pip install codecarbon\n",
        "# !pip install dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqyN-7TcXbL8"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Sqih7m6tN4MT"
      },
      "outputs": [],
      "source": [
        "import requests, zipfile, io\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "from typing import List, Dict\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import pandas as pd\n",
        "from codecarbon import EmissionsTracker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHGGrr3GXdIb"
      },
      "source": [
        "# Endpoints\n",
        "These URL addresses are necessary for the connection to the server.\n",
        "\n",
        "**IMPORTANT:** Replace \"URL\" by the URL server and \"TOKEN\" by your user token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdQPl8lbOKsg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "http://s3-ceatic.ujaen.es:8036 c461869975ffb0a7ba8544ffdddf3b58\n"
          ]
        }
      ],
      "source": [
        "load_dotenv()\n",
        "URL = os.getenv(\"SERVER_URL\")\n",
        "TOKEN = os.getenv(\"ACCESS_TOKEN\")\n",
        "print(URL, TOKEN)\n",
        "\n",
        "# Download endpoints\n",
        "ENDPOINT_DOWNLOAD_TRIAL = URL+\"/{TASK}/download_trial/{TOKEN}\"\n",
        "ENDPOINT_DOWNLOAD_TRAIN = URL+\"/{TASK}/download_train/{TOKEN}\"\n",
        "\n",
        "# Trial endpoints\n",
        "ENDPOINT_GET_MESSAGES_TRIAL = URL+\"/{TASK}/getmessages_trial/{TOKEN}\"\n",
        "ENDPOINT_SUBMIT_DECISIONS_TRIAL = URL+\"/{TASK}/submit_trial/{TOKEN}/{RUN}\"\n",
        "\n",
        "# Test endpoints\n",
        "ENDPOINT_GET_MESSAGES = URL+\"/{TASK}/getmessages/{TOKEN}\"\n",
        "ENDPOINT_SUBMIT_DECISIONS = URL+\"/{TASK}/submit/{TOKEN}/{RUN}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgHNiyxHR5AJ"
      },
      "source": [
        "# Download Data\n",
        "To download the data, you can make use of the **function defined in the following**.\n",
        "\n",
        "The following function download the trial data. To adapt it to download the train and test data, follow the instructions given in the [website of the competition](https://sites.google.com/view/mentalriskes2024/evaluation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Uaeh23C5R1lG"
      },
      "outputs": [],
      "source": [
        "def download_messages_trial(task: str, token: str):\n",
        "    \"\"\" Allows you to download the trial data of the task.\n",
        "        Args:\n",
        "          task (str): task from which the data is to be retrieved\n",
        "          token (str): authentication token\n",
        "    \"\"\"\n",
        "\n",
        "    response = requests.get(ENDPOINT_DOWNLOAD_TRIAL.format(TASK=task, TOKEN=token))\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(\"Trial - Status Code \" + task + \": \" + str(response.status_code) + \" - Error: \" + str(response.text))\n",
        "    else:\n",
        "      z = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "      os.makedirs(\"./data/{task}/trial/\".format(task=task))\n",
        "      z.extractall(\"./data/{task}/trial/\".format(task=task))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v09j7ChSD6c-"
      },
      "outputs": [],
      "source": [
        "def download_messages_train(task: str, token: str):\n",
        "    \"\"\" Allows you to download the train data of the task.\n",
        "        Args:\n",
        "          task (str): task from which the data is to be retrieved\n",
        "          token (str): authentication token\n",
        "    \"\"\"\n",
        "    response = requests.get(ENDPOINT_DOWNLOAD_TRAIN.format(TASK=task, TOKEN=token))\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(\"Train - Status Code \" + task + \": \" + str(response.status_code) + \" - Error: \" + str(response.text))\n",
        "    else:\n",
        "      z = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "      os.makedirs(\"./data/{task}/train/\".format(task=task),exist_ok=True)\n",
        "      z.extractall(\"./data/{task}/train/\".format(task=task))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\SIMBAT\\anaconda3\\envs\\DL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\SIMBAT\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\SIMBAT\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\SIMBAT\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\SIMBAT\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\SIMBAT\\anaconda3\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Required basic imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import string\n",
        "import joblib\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# Imports for tokenizing and vectorizing data\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Imports for vectorized data pre-processing and classification models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import unicodedata\n",
        "\n",
        "# Imports for Hugging Face BERT transformers and AutoTokenizers\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "def clear_tokens(tokens_list: list) -> None:\n",
        "    \"\"\" Removes punctuation symbols from the given tokens list \"\"\"\n",
        "    # Traverse the list backwards to avoid logic problems with pop() and indexes order\n",
        "    for i in range(len(tokens_list) - 1, -1, -1):\n",
        "        if tokens_list[i] in string.punctuation:\n",
        "            tokens_list.pop(i)\n",
        "    return\n",
        "\n",
        "def remove_emojis(keyword):\n",
        "    return re.sub(r\"[^\\w\\s,!?@#áéíóúÁÉÍÓÚñÑ]\", \"\", keyword)\n",
        "\n",
        "\n",
        "def user_document_to_input(messages_list, trained_vectorizer):\n",
        "    tokenizer = TweetTokenizer()\n",
        "    \n",
        "    # Remove emojis and tokenize\n",
        "    tokens = tokenizer.tokenize(remove_emojis(\" \".join(messages_list)))\n",
        "    clear_tokens(tokens)\n",
        "    \n",
        "    # Join tokens into a single document string\n",
        "    user_document = \" \".join(tokens)\n",
        "    \n",
        "    X = trained_vectorizer.transform([user_document])\n",
        "    return pd.DataFrame(X.toarray(), columns=trained_vectorizer.get_feature_names_out())\n",
        "\n",
        "\n",
        "# load the final models\n",
        "# --- tfidf vectorizer:\n",
        "vectorizer_tfidf_task1 = joblib.load(\"trained_models/tfidf_vectorizer_task1_emojis.joblib\")\n",
        "vectorizer_tfidf_task2 = joblib.load(\"trained_models/tfidf_vectorizer_task2_emojis.joblib\")\n",
        "\n",
        "# --- random forest baseline task 2\n",
        "model_rf_task2 = joblib.load(\"trained_models/RF_task2_emojis.joblib\")\n",
        "\n",
        "# --- svm baseline task 1\n",
        "model_svm_task1 = joblib.load(\"trained_models/SVM_task1_emojis.joblib\")\n",
        "\n",
        "# --- dedicate svms task 1\n",
        "topics = ['betting', 'trading', 'onlinegaming', 'lootboxes']\n",
        "svm_models_task1 = {\n",
        "    k: joblib.load(f\"trained_models/SVM_task1_{k}.joblib\") for k in topics\n",
        "}\n",
        "\n",
        "# --- BERT-small model task 1\n",
        "bert_task1            = AutoModelForSequenceClassification.from_pretrained(f\"./task1_small_bert\")\n",
        "tokenizer_bert_task1  = AutoTokenizer.from_pretrained(f\"./task1_small_bert\")\n",
        "\n",
        "# ---BERT-small model task 2\n",
        "bert_task2            = AutoModelForSequenceClassification.from_pretrained(f\"./task2_small_bert\")\n",
        "tokenizer_bert_task2  = AutoTokenizer.from_pretrained(f\"./task2_small_bert\")\n",
        "\n",
        "# Define task1 and task2 labels\n",
        "task1_labels = { '0': 0, '1': 1 }\n",
        "task2_labels = {  0: 'betting', 1: 'lootboxes', 2: 'onlinegaming', 3: 'trading'  }\n",
        "\n",
        "class Pipeline:\n",
        "    def predict(self, user_input):\n",
        "        rf_pred = model_rf_task2.predict(user_input)\n",
        "        return svm_models_task1[rf_pred[0]].predict(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_text(model, tokenizer, text: str, labels: dict):\n",
        "    \"\"\" Make predictions with BERT Transformer model \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        pred_label = torch.argmax(logits, dim=1).item()\n",
        "    return labels[pred_label]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIqRCv3OS3Bn"
      },
      "source": [
        "# Client Server\n",
        "This class simulates communication with our server. The following code established the conection with the server client and simulate the GET and POST requests.\n",
        "\n",
        "**IMPORTANT NOTE:** Please pay attention to the basic functions and remember that it is only a base for your system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "l0kONpltS2R9"
      },
      "outputs": [],
      "source": [
        "class Client_task1_2:\n",
        "    \"\"\" Client communicating with the official server.\n",
        "        Attributes:\n",
        "            token (str): authentication token\n",
        "            number_of_runs (int): number of systems. Must be 3 in order to advance to the next round.\n",
        "            tracker (EmissionsTracker): object to calculate the carbon footprint in prediction\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, task:str, token: str, number_of_runs: int, tracker: EmissionsTracker):\n",
        "        self.task = task\n",
        "        self.token = token\n",
        "        self.number_of_runs = number_of_runs\n",
        "        self.tracker = tracker\n",
        "        self.relevant_cols = ['duration', 'emissions', 'cpu_energy', 'gpu_energy',\n",
        "                              'ram_energy','energy_consumed', 'cpu_count', 'gpu_count',\n",
        "                              'cpu_model', 'gpu_model', 'ram_total_size','country_iso_code']\n",
        "\n",
        "\n",
        "    def get_messages(self, retries: int, backoff: float) -> Dict:\n",
        "        \"\"\" Allows you to download the test data of the task by rounds.\n",
        "            Here a GET request is sent to the server to extract the data.\n",
        "            Args:\n",
        "              retries (int): number of calls on the server connection\n",
        "              backoff (float): time between retries\n",
        "        \"\"\"\n",
        "        session = requests.Session()\n",
        "        retries = Retry(\n",
        "                        total = retries,\n",
        "                        backoff_factor = backoff,\n",
        "                        status_forcelist = [500, 502, 503, 504]\n",
        "                        )\n",
        "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "        response = session.get(ENDPOINT_GET_MESSAGES_TRIAL.format(TASK=self.task, TOKEN=self.token)) # ENDPOINT\n",
        "\n",
        "        if response.status_code != 200:\n",
        "          print(\"GET - Task {} - Status Code {} - Error: {}\".format(self.task, str(response.status_code), str(response.text)))\n",
        "          return []\n",
        "        else:\n",
        "          return json.loads(response.content)\n",
        "\n",
        "    def submit_decission(self, messages: List[Dict], emissions: Dict, retries: int, backoff: float):\n",
        "        \"\"\" Allows you to submit the decisions of the task by rounds.\n",
        "            The POST requests are sent to the server to send predictions and carbon emission data\n",
        "            Args:\n",
        "              messages (List[Dict]): Message set of the current round\n",
        "              emissions (Dict): carbon footprint generated in the prediction\n",
        "              retries (int): number of calls on the server connection\n",
        "              backoff (float): time between retries\n",
        "        \"\"\"\n",
        "        decisions_run0 = {}\n",
        "        decisions_run1 = {}\n",
        "        decisions_run2 = {}\n",
        "        type_addiction_list = [\"betting\", \"onlinegaming\", \"betting\", \"trading\"]\n",
        "        type_addiction_decision = {}\n",
        "\n",
        "        # You must create the appropriate structure to send the predictions according to each task\n",
        "        for message in messages:\n",
        "            decisions_run0[message[\"nick\"]] = messages[\"run0_svm\"] # random.choice([0,1])\n",
        "            decisions_run1[message[\"nick\"]] = messages[\"run1_bert\"] # random.choice([0,1])\n",
        "            decisions_run2[message[\"nick\"]] = messages[\"run2_pipeline\"] # random.choice([0,1])\n",
        "            type_addiction_decision[message[\"nick\"]] = messages[\"type_addiction\"] # random.choice(type_addiction_list)\n",
        "\n",
        "        data1_run0 = {\n",
        "            \"predictions\": decisions_run0, \n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "        data1_run1 = {\n",
        "            \"predictions\": decisions_run1,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "        data1_run2 = {\n",
        "            \"predictions\": decisions_run2,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "        data2_run0 = {\n",
        "            \"predictions\": decisions_run0,\n",
        "            \"types\":type_addiction_decision,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "        data2_run1 = {\n",
        "            \"predictions\": decisions_run1,\n",
        "            \"types\":type_addiction_decision,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "        data2_run2 = {\n",
        "            \"predictions\": decisions_run2,\n",
        "            \"types\":type_addiction_decision,\n",
        "            \"emissions\": emissions\n",
        "        }\n",
        "\n",
        "        data1 = []\n",
        "        data1.append(json.dumps(data1_run0))\n",
        "        data1.append(json.dumps(data1_run1))\n",
        "        data1.append(json.dumps(data1_run2))\n",
        "\n",
        "        data2 = []\n",
        "        data2.append(json.dumps(data2_run0))\n",
        "        data2.append(json.dumps(data2_run1))\n",
        "        data2.append(json.dumps(data2_run2))\n",
        "\n",
        "        # Session to POST request\n",
        "        session = requests.Session()\n",
        "        retries = Retry(\n",
        "                        total = retries,\n",
        "                        backoff_factor = backoff,\n",
        "                        status_forcelist = [500, 502, 503, 504]\n",
        "                        )\n",
        "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "        for run in range(0, self.number_of_runs):\n",
        "            # For each run, new decisions\n",
        "            response1 = session.post(ENDPOINT_SUBMIT_DECISIONS_TRIAL.format(TASK='task1', TOKEN=self.token, RUN=run), json=[data1[run]]) # ENDPOINT\n",
        "            if response1.status_code != 200:\n",
        "                print(\"POST - Task1 - Status Code {} - Error: {}\".format(str(response1.status_code), str(response1.text)))\n",
        "                return\n",
        "            else:\n",
        "                print(\"POST - Task1 - run {} - Message: {}\".format(run, str(response1.text)))\n",
        "\n",
        "            response2 = session.post(ENDPOINT_SUBMIT_DECISIONS_TRIAL.format(TASK='task2', TOKEN=self.token, RUN=run), json=[data2[run]]) # ENDPOINT\n",
        "            if response2.status_code != 200:\n",
        "                print(\"POST - Task2 - Status Code {} - Error: {}\".format(str(response2.status_code), str(response2.text)))\n",
        "                return\n",
        "            else:\n",
        "                print(\"POST - Task2 - run {} - Message: {}\".format(run, str(response2.text)))\n",
        "\n",
        "            with open('./data/preds/task1/round{}_run{}.json'.format(messages[0][\"round\"], run), 'w+', encoding='utf8') as json_file:\n",
        "                json.dump(data1[run], json_file, ensure_ascii=False)\n",
        "            with open('./data/preds/task2/round{}_run{}.json'.format(messages[0][\"round\"], run), 'w+', encoding='utf8') as json_file:\n",
        "                json.dump(data2[run], json_file, ensure_ascii=False)\n",
        "\n",
        "\n",
        "    def run_task1_2(self, retries: int, backoff: float):\n",
        "        \"\"\" Main thread\n",
        "            Args:\n",
        "              retries (int): number of calls on the server connection\n",
        "              backoff (float): time between retries\n",
        "        \"\"\"\n",
        "        # Get messages for task1_2\n",
        "        messages = self.get_messages(retries, backoff)\n",
        "\n",
        "        # If there are no messages\n",
        "        if len(messages) == 0:\n",
        "            print(\"All rounds processed\")\n",
        "            return\n",
        "\n",
        "        # list of messages per user\n",
        "        user_messages = {m[\"nick\"]: [m[\"message\"]] for m in messages}\n",
        "        round_count = 1\n",
        "        submitted = False\n",
        "\n",
        "        while len(messages) > 0:\n",
        "            print(messages)\n",
        "            print(\"----------------------- Processing round {}\".format(messages[0][\"round\"]))\n",
        "            # Save subjects\n",
        "            with open('./data/rounds/round{}.json'.format(messages[0][\"round\"]), 'w+', encoding='utf8') as json_file:\n",
        "                json.dump(messages, json_file, ensure_ascii=False)\n",
        "\n",
        "\n",
        "            if round_count > 49:\n",
        "                # Calculate emissions for each prediction\n",
        "                self.tracker.start()\n",
        "\n",
        "                user_predictions = []\n",
        "                for user_id, user_messages_list in user_messages.items():\n",
        "                    user_input     = user_document_to_input(user_messages_list, vectorizer_tfidf_task2)\n",
        "                    run0_svm       = int(model_svm_task1.predict(user_input)[0])\n",
        "                    run1_bert      = predict_text(bert_task1, tokenizer_bert_task1, \" \".join(user_messages_list), task1_labels)\n",
        "                    run2_pipeline  = int(Pipeline().predict(user_input)[0])\n",
        "                    type_addiction = str(model_rf_task2.predict(user_input)[0])\n",
        "\n",
        "                    user_predictions.append(\n",
        "                        {\n",
        "                            \"round\": messages[0][\"round\"],\n",
        "                            \"nick\": user_id,\n",
        "                            \"run0_svm\": run0_svm,\n",
        "                            \"run1_bert\": run1_bert,\n",
        "                            \"run2_pipeline\": run2_pipeline,\n",
        "                            \"type_addiction\": type_addiction,\n",
        "                        }\n",
        "                    )\n",
        "                emissions = self.tracker.stop()\n",
        "\n",
        "                df = pd.read_csv(\"emissions.csv\")\n",
        "                measurements = df.iloc[-1][self.relevant_cols].to_dict()\n",
        "\n",
        "                self.submit_decission(user_predictions, measurements, retries, backoff)\n",
        "                submitted = True\n",
        "                break\n",
        "\n",
        "            # One GET request for each round\n",
        "            messages = self.get_messages(retries, backoff)\n",
        "            round_count += 1\n",
        "\n",
        "            # accumulate next message\n",
        "            for message in messages:\n",
        "                user_messages[message[\"nick\"]].append(message[\"message\"])\n",
        "\n",
        "        if not submitted:\n",
        "            self.tracker.start()\n",
        "           \n",
        "            user_predictions = []\n",
        "            for user_id, user_messages_list in user_messages.items():\n",
        "                user_input     = user_document_to_input(user_messages_list, vectorizer_tfidf_task2)\n",
        "                run0_svm       = int(model_svm_task1.predict(user_input)[0])\n",
        "                run1_bert      = predict_text(bert_task1, tokenizer_bert_task1, \" \".join(user_messages_list), task1_labels) \n",
        "                run2_pipeline  = int(Pipeline().predict(user_input)[0])\n",
        "                type_addiction = str(model_rf_task2.predict(user_input)[0])\n",
        "\n",
        "                user_predictions.append(\n",
        "                    {\n",
        "                        \"round\": messages[0][\"round\"],\n",
        "                        \"nick\": user_id,\n",
        "                        \"run0_svm\": run0_svm,\n",
        "                        \"run1_bert\": run1_bert,\n",
        "                        \"run2_pipeline\": run2_pipeline,\n",
        "                        \"type_addiction\": type_addiction,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            emissions = self.tracker.stop()\n",
        "            df = pd.read_csv(\"emissions.csv\")\n",
        "            measurements = df.iloc[-1][self.relevant_cols].to_dict()\n",
        "            self.submit_decission(user_predictions, measurements, retries, backoff)\n",
        "            submitted = True\n",
        "\n",
        "        print(\"All rounds processed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMXuHLciXIO3"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZrDpxNAS6-3"
      },
      "outputs": [],
      "source": [
        "def download_data(task: str, token: str):\n",
        "    #download_messages_trial(task, token)\n",
        "    download_messages_train(task, token)\n",
        "\n",
        "def get_post_data(task: str, token: str):\n",
        "    # Emissions Tracker Config\n",
        "    config = {\n",
        "        \"save_to_file\": True,\n",
        "        \"log_level\": \"WARNING\",\n",
        "        \"tracking_mode\": \"process\",\n",
        "        \"output_dir\": \".\",\n",
        "        \"allow_multiple_runs\": True\n",
        "    }\n",
        "    tracker = EmissionsTracker(**config)\n",
        "\n",
        "    number_runs = 3 # Max: 3\n",
        "\n",
        "    # Prediction period\n",
        "    client_task1_2 = Client_task1_2(task, token, number_runs, tracker)\n",
        "    client_task1_2.run_task1_2(5, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff6QgM3gErMm"
      },
      "source": [
        "Be careful! In this specific example we use the name of the task1 to do the get, knowing that it is the same data for both task 1 and task 2. In addition, the data upload is performed for both tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "aKMWQ5buS8OK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon WARNING @ 21:49:17] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 21:49:17] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 21:49:18] We saw that you have a 12th Gen Intel(R) Core(TM) i7-12700F but we don't know it. Please contact us.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GET - Task trial - Status Code 404 - Error: {\"detail\":\"Not Found\"}\n",
            "All rounds processed\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    #download_data(\"task1\", TOKEN)\n",
        "    get_post_data(\"trial\", TOKEN)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "DL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
